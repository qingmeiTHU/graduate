\chapter{主播端的视频质量问题}
在这一章中，我们通过测量一些主流的个人直播应用，发现了主播端的质量下降问题。之后我们仔细分析了问题背后的原因，提出了可能的解决方案。

\section{主播端性能测量}
\textbf{实验设置} 我们搭建了如图2所示的直播传输架构。 两端的设备（主播端和观众端）都有两个CPU核，6G内存，装备着100Mbps的网卡，他们通过中间的交换机连接在一起。主播端用OBS去发送视频，OBS是一个主流的使用RTMP协议的直播软件。另外，主播端有个带宽控制的模块，模拟无线环境的网络带宽变化。带宽控制模块是用Linux的tc模块和dummynet来实现的。观众端使用nginx-rtmp模块搭配rtmp接收服务器，接收视频，并用VLC播放器播放接收到的RTMP流。

\subsection{案例分析：变化带宽下性能不佳}
我们根据无线网络下的真实数据去控制网络带宽变化。数据记录的是当一个用户通过移动设备浏览亚马逊网站时的实时带宽。我们按照5秒的时间间隔去统计发送的总数据包数，然后计算每个时隙的总数据量。我们根据带宽的数据去控制主播端的网络带宽，数据记录的平均带宽大于3300kbps。 开始我们把码率设为3300kbps，用OBS去上传视频，并在观众端用tcpdump去抓取报文。结果如下图所示。数据记录持续时间为320s，我们把记录分为两部分，0-140s以及120-260s。

从实验图，我们得出两个结论。（1）在图1中，实际带宽紧紧跟随带宽数据变化，然而，在50s时，带宽下降到码率以下，这种情况维持了2s，然而真实的吞吐量降为0，维持了8s，从50s到58s。这是一种不正常的现象，2s的网络抖动导致了直播流8的吞吐量下降。（2）从图2可以看出固定的码率不能有效的解决长时间的带宽变化。0-180s带宽是充足的，但是180s之后带宽急剧下降，这个过程持续了80s，在这期间发生了大量的丢帧。在这个很差的网络环境下，OBS算法依然默认设置之前的码率，这个策略显示不够令人满意。

\subsection{带宽波动普遍存在}
我们通过分析两个真实数据集的带宽分布来预估带宽抖动发生的频率。两个数据集分别是FCC数据集和HSDPA数据集，每个记录持续320s，数据集共持续了30多小时。对于每一个记录来说，我们根据平均带宽去归一化记录，画出了它的cdf图。50\%左右的记录在平均带宽以下，这意味着对于一个10s的带宽记录来说，有5s的时间，带宽会低于平均值。20\%的记录只有平均值的一半。该图表明在现实世界的网络中，带宽波动频繁发生。为了进一步说明带宽抖动持续的时长，我们画了一张带宽抖动时长分布的图。带宽抖动时长等于低于平均值的持续时间。大约20\%的网络抖动持续时长多于10s，有些甚至持续数百秒。另外，我们还可以从上面的两个图中看出，FCC数据集和HSDPA数据集的分布和整体类似。

\subsection{商业平台验证}

\section{追溯根本原因}

\section{优化空间}